#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
MAAT-KI â€” Model Downloader
--------------------------
Automatischer Download des empfohlenen Modells:

Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf
Quelle:
GPT4All-Community (HuggingFace)

âš ï¸ Hinweis:
Dieses Modell wird als leistungsfÃ¤hig angenommen, benÃ¶tigt aber
mÃ¶glicherweise weitere Tests und Nutzer-Feedback.
Nur Modelle, die offiziell in der GPT4All-GUI angezeigt werden,
sind von Nomic verifiziert. Nutzung auf eigene Gefahr.
"""

import os
import shutil
import requests

MODEL_URL = (
    "https://huggingface.co/GGPT4All-Community/Meta-Llama-3.1-8B-Instruct-128k-GGUF/"
    "resolve/main/Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf?download=true"
)

MODEL_NAME = "Meta-Llama-3.1-8B-Instruct-128k-Q4_0.gguf"


def download_model():
    print("\nğŸŒ¿ MAAT-KI Model Downloader")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ“¦ Modell: {MODEL_NAME}")
    print(f"ğŸ”— Quelle: {MODEL_URL}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n")

    root_path = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(root_path, "models")

    if not os.path.exists(models_dir):
        os.makedirs(models_dir)

    target_path = os.path.join(models_dir, MODEL_NAME)
    temp_path = os.path.join(root_path, MODEL_NAME)

    # Wenn Modell schon existiert
    if os.path.exists(target_path):
        print("âœ”ï¸ Modell bereits im models/ Verzeichnis vorhanden.")
        return

    print("â³ Lade Modell herunterâ€¦ (dies kann dauern)")

    with requests.get(MODEL_URL, stream=True) as r:
        r.raise_for_status()
        with open(temp_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

    print(f"âœ”ï¸ Download abgeschlossen: {temp_path}")

    # Kopieren ins models/
    print("ğŸ“ Verschiebe Modell in das models/ Verzeichnisâ€¦")
    shutil.move(temp_path, target_path)

    print("ğŸŒŸ Fertig! Modell ist bereit fÃ¼r MAAT-KI.")
    print(f"â¡ï¸ {target_path}")


if __name__ == "__main__":
    download_model()